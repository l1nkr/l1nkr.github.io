<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"l1nkr.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="google-file-system 论文阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="google-file-system 论文阅读">
<meta property="og:url" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/index.html">
<meta property="og:site_name" content="变秃之路">
<meta property="og:description" content="google-file-system 论文阅读">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-file-chunk.jpg">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-file-storage.jpg">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-architecture.jpg">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-consistency-model.jpg">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-write-control.jpg">
<meta property="og:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-append-problem.jpg">
<meta property="article:published_time" content="2021-12-15T09:06:27.000Z">
<meta property="article:modified_time" content="2021-12-15T09:08:43.701Z">
<meta property="article:author" content="l1nkr">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://l1nkr.github.io/2021/12/15/google-file-system-md/gfs-file-chunk.jpg">

<link rel="canonical" href="https://l1nkr.github.io/2021/12/15/google-file-system-md/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>google-file-system 论文阅读 | 变秃之路</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="变秃之路" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">变秃之路</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">变秃之路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://l1nkr.github.io/2021/12/15/google-file-system-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/ye.jpg">
      <meta itemprop="name" content="l1nkr">
      <meta itemprop="description" content="Work it harder. Make it better">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="变秃之路">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          google-file-system 论文阅读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-15 17:06:27 / 修改时间：17:08:43" itemprop="dateCreated datePublished" datetime="2021-12-15T17:06:27+08:00">2021-12-15</time>
            </span>

          
            <div class="post-description">google-file-system 论文阅读</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Google-File-System-论文阅读"><a href="#Google-File-System-论文阅读" class="headerlink" title="Google File System 论文阅读"></a>Google File System 论文阅读</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><ul>
<li>分布式组件经常发生错误，应当将此视为常态而不是意外</li>
<li>文件通常是大文件，而不是小文件</li>
<li>大部分文件（指字节数量占比高，而不是操作次数）通过 append 的方式实现修改，而不是直接重写现有数据</li>
<li>协同设计应用以及文件系统的 api 可以提高系统整体灵活性，最终使整个系统收益</li>
</ul>
<h3 id="Design-Overview"><a href="#Design-Overview" class="headerlink" title="Design Overview"></a>Design Overview</h3><h4 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h4><ul>
<li>系统由许多廉价的普通组件组成，组件失效是一种常态</li>
<li>系统存储一定数量的大文件</li>
<li>系统的工作负载主要由两种读操作组成:<strong>大规模的流式读取和小规模的随机读取</strong></li>
<li>系统的工作负载还包括大规模的、顺序的、数据追加方式的写操作</li>
<li>系统必须高效的、行为定义明确的实现多客户端并行追加数据到同一个文件里的语义</li>
<li>高持续带宽（High sustained bandwidth）比低延迟更重要</li>
</ul>
<h4 id="Interface"><a href="#Interface" class="headerlink" title="Interface"></a>Interface</h4><p>GFS 作为一个分布式文件系统，对外提供了一个传统的单机文件系统接口。但是出于效率和使用性的角度，并没有实现标准的文件系统 POSIX API<br>GFS 还支持如下两个特性：</p>
<ul>
<li>Snapshot 快照：快照指的是以低成本方式创建文件和目录树（directory tree）的副本；</li>
<li>Record Append 记录追加：记录追加指的是 GFS 允许多个客户机并发安全地向同一文件追加数据，同时保证每个客户追加操作的原子性</li>
</ul>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p>一个 GFS cluster（集群）分为两个组件：</p>
<ul>
<li>单个 master 节点；</li>
<li>多个 chunkserver 节点；<br>一个 GFS 集群同时可以被多个 client（客户）节点访问。</li>
</ul>
<h4 id="Chunk-size"><a href="#Chunk-size" class="headerlink" title="Chunk size"></a>Chunk size</h4><p>原本一个磁盘转速 5400 rpm，那么读完一个 10 GB 的文件可能需要 200 秒（假设），现在 10 GB 的文件分别在 10 个磁盘上存储，转速不变，那么文件读完仅仅需要 20 秒。这当然是理想情况，但是可以见得大文件分块存储后对系统总的读写速度的提升。</p>
<p>Chunk Size 是整个分布式文件系统的最重要的参数之一，<strong>GFS 以 64 MB 为固定的 Chunk Size 大小</strong></p>
<p>每一个文件都被划分为多个 chunk</p>
<p><img src="/2021/12/15/google-file-system-md/gfs-file-chunk.jpg"></p>
<p>选取大 chunk size 有如下优点：</p>
<ul>
<li>它减少了 Client 与 Master 服务器交互的次数，对同一块进行多次读写仅仅需要向 Master 服务器发出一次初始请求</li>
<li>减少了 GFS Client 与 GFS chunkserver 进行交互的数据开销，能够降低网络开销，因为数据的读取具有连续读取的倾向</li>
<li>它减少了存储在主服务器上的元数据的大小。这允许我们将元数据保存在内存中</li>
</ul>
<p>缺点是：</p>
<ul>
<li>可能会造成 hot-spot。chunk size 较大时，文件包含较少的Chunk，甚至只有一个Chunk。当有许多的客户端对同一个小文件进行多次的访问时，存储这些Chunk的Chunk服务器就会变成热点</li>
</ul>
<p>分布式系统由于不可避免的故障，因此我们需要使用 replication 机制，每一个 chunk 都存在着若干个副本（它们不一定完全一样 ，因为 GFS 并不是一个强一致性文件管理系统），我们称这些 chunk 的副本为 replica（复数形式为 replicas）。每个 chunk 或者 replica 都作为普通的 Linux 文件存储在 chunkserver 上。</p>
<p> GFS 分布式系统架构的<strong>分布式</strong>结构以及<strong>文件的分块存储特性</strong></p>
<p><img src="/2021/12/15/google-file-system-md/gfs-file-storage.jpg"></p>
<h4 id="Single-Master"><a href="#Single-Master" class="headerlink" title="Single Master"></a>Single Master</h4><p>GFS Master 节点负责的工作是系统级别的控制，主要有：</p>
<ul>
<li>chunk lease manager；</li>
<li>garbage collection of orphaned chunks；</li>
<li>chunk migration between chunkservers；</li>
<li>append record 原子性的确保；</li>
<li>对于 metadata 的维护以及为请求 metadata 数据的 client 做出正确的响应；</li>
</ul>
<p>Master 周期性通过 HeartBeat 机制和每一个 chunkserver 进行通信，进行指令的发送以及状态信息的接收。</p>
<p>Master 节点基于 Master-Worker 模式设计，Client 读写操作的数据 I/O 传输直接与 chunkserver 进行。</p>
<p>同时，GFS 实际上仅仅拥有一个 Master 服务器，这极大地简化了设计难度，做出快速的决策。当然，这样也会引入问题。比如 GFS 的 Master 节点在故障以后，并没有自我恢复功能，虽然存在 shadow master 节点，但是它们并没有晋升为真正 Master 节点的功能（至少 GFS 论文中没有指出），Master 节点需要人工地进行故障恢复，这会导致小时级别的 GFS 系统不可写（但是 shadow master 节点支持数据读取功能）；</p>
<p><img src="/2021/12/15/google-file-system-md/gfs-architecture.jpg"></p>
<p>从 GFS Master 返回的是 chunk locations，表示 replicas，即多个 replica 的地址。</p>
<p>如果缓存没有过期，那么 GFS Client 与同一个 replica 对同一 chunk 的读并不需要 clinet-master 进行通信；</p>
<p>同时，客户端通常会对多个 chunk 进行合理的聚合，可以一次向 master 查询多个 chunk + index 的 metadata，以及一次向 chunkserver 读取多个 chunk 的数据；</p>
<p>Master 节点<strong>在内存中存储</strong>着两个 Table，它们被统称为 metadata，它们存储的内容如下：</p>
<ul>
<li>Table 1：<ul>
<li>key：file name</li>
<li>value：<strong>an array of</strong> chunk handler (nv)</li>
</ul>
</li>
<li>Table 2：<ul>
<li>key：chunk handler</li>
<li>value：<ul>
<li><strong>a list of</strong> chunkserver(v)</li>
<li>chunk version number(nv)</li>
<li>which chunkserver is primary node(which means others are nomal chunkserver in the list)(v)</li>
<li>lease expiration time(v)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上面 nv 含义是 Non-Volatile，也就是非易失性的含义，即要求将数据存储到磁盘上，而 v 的含义便是 Volatile，这些数据不需要持久化，当 Master 节点重启时，通过和每一个 chunkserver 进行通信来初始化不需要持久化的数据。</p>
<p>metadata 存储于 master 内存中，但是因为内存是易失性存储体，因此还需要持久化操作</p>
<ul>
<li>Master 节点使用 log + checkpoint 的方式来确保数据的持久化；</li>
<li>Master 节点内存中的 Table1 与 Table2 中的 nv 数据会定期存储到 Master 的磁盘上（包括 shadow Master 节点）</li>
</ul>
<p><strong>为什么有 Table1 与 Table2 的持久化机制了，还需要日志和 checkpoint？</strong></p>
<p>因为 Table1 与 Table2 的持久化机制是定期执行的。Table1 与 Table2 的数据结构无论是 HashMap 还是 B+Tree，如果选择新加入节点后马上进行持久化，那么就会面临随机 I/O 的问题，因为它们本质上都基于节点实现，而节点并不基于连续地址进行存储。所以出于效率的考虑，这两张表并不会在每一个写操作执行时就执行持久化机制，而是定期执行。但是定期执行就存在因为掉电、故障后数据丢失问题，因此需要引入日志系统。这通常被称为持久化 snapshot of memory。</p>
<p>由于日志仅仅就是追加数据，日志的追加操作属于顺序磁盘 I/O，因此每一条写操作生效前都可以提前把日志记录到磁盘上，再进行真正的写操作，因此数据总是能够安全地确保持久化。主机重启时重新执行一遍日志即可。</p>
<p>引入 checkpoint 的原因是日志系统非常冗长，如果每次启动 Master 节点时都执行全部的日志记录一次，那么效率就会很低。另一方面，GFS 本身就必然会持久化 Table1、Table2，我们应当利用这个特点。checkpoint 就是来解决这个问题的。每次持久化 Table1、Table2 成功后，都会在日志系统上打上一个 checkpoint，用于说明下一次启动 master 之后可以先读取持久化了的 Table1、Table2 磁盘数据，然后从日志系统的 checkpoint 向后执行。这样一来 Master 启动时的效率就不会很低了，因为并不会将日志从头到尾执行一遍，而仅仅是 checkpoint 到尾执行一遍。另一面，checkpoint 也赋予了日志系统删除陈旧的日志的能力，用来节约磁盘空间（checkpoint 前的字节数据理论上都可以删除）。</p>
<h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><p>Master 存储三类最重要的 metadata 数据</p>
<ul>
<li>File 和 chunk 的 namespace(命名空间)</li>
<li>File 到 chunk 的 Map</li>
<li>每一个 chunk replica 的存储位置</li>
</ul>
<p>前两种类型(名称空间和文件到块的映射)数据同时也会通过 logging metuations 存储到主机本地磁盘上、复制到远程机器上来持久化。</p>
<p>Master 服务器不持久化 chunkservers 拥有哪个 chunk 副本的持久记录，因为chunkserver 对它自己的磁盘上的块有最终决定权，试图在 Master 服务器上维护这个信息的一致视图是没有意义的，获取该信息通过询问实现：</p>
<ul>
<li>Master 在启动时通过轮询获取每一个 chunkserver 上所有 chunk replica 的初始值；</li>
<li>Master 在启动后通过心跳机制来检测每一个 chunkserver 上所有 chunk replica 的变化，以保证其拥有最新的数据。</li>
</ul>
<p><strong>Operation Log</strong> 包含关键 metadata 数据更改的历史记录。它是 GFS 系统的核心。它不仅是元数据的唯一持久记录，而且还充当定义并发操作顺序的逻辑时间线。</p>
<p>由于 <strong>Operation Log</strong> 日志非常重要，所以我们必须可靠地存储它，并且在元数据的更改被持久化之前，不能使更改对 client 可见，否则我们会丢失最近的 client 操作。因此，我们将它复制到多个远程机器上，只有在<strong>本地和远程</strong>将相应的日志记录刷新到磁盘之后才响应客户机操作。在刷新之前，Master 批处理多个日志记录，从而减少刷新和复制对总体系统吞吐量的影响。</p>
<p>Master 通过重播操作日志恢复其文件系统状态。为了最小化启动时间，我们必须保持日志较小。每当日志增长超过一定的大小时，Master 检查其状态，以便通过从本地磁盘加载最新的 checkpoint 并在此之后仅重放有限数量的日志记录来恢复。</p>
<h4 id="Consistency-Model"><a href="#Consistency-Model" class="headerlink" title="Consistency Model"></a>Consistency Model</h4><p><img src="/2021/12/15/google-file-system-md/gfs-consistency-model.jpg"></p>
<p>Failure 表示并发修改失败，Concurrent success 表示并发修改成功，Serial success 则表示串行修改成功，串行要求最高，但是其如同单线程写一样不存在任何并发问题。</p>
<p> GFS 系统中对 file region 状态的概念定义：</p>
<ul>
<li>consistent：所有 GFS Client 将总是看到完全相同的数据，无论 GFS Client 最终是从哪一个 GFS chunkserver replica 上进行数据读取；</li>
<li>defined：当一个文件数据修改之后如果 file region 还是保持 consistent 状态，并且所有 client 能够看到全部修改（且已经写入 chunkserver）的内容；</li>
<li>consistent but undefined：从定义上来看，就是所有 client 能够看到相同的数据，但是并不能及时反映并发修改中的任意修改；</li>
<li>inconsistent：因为处于 inconsistent 状态，因此一定也处于 undeﬁned 状态，造成此状态的操作也被认为是 failed 的。不同的 Client 在能读到的数据不一致，同一个 Client 在不同的时刻读取的文件数据也不一致。</li>
</ul>
<p>其次，表格将数据的修改分为两种情况：</p>
<ul>
<li>Write：修改 File 中的原有数据，具体来说就是在指定文件的偏移地址下写入数据（覆写）；</li>
<li>Record Append：即在原有 File 末尾 Append(追加)数据，这种操作被 GFS 系统确保为原子操作，这是 GFS 系统最重要的优化之一。GFS 中的 append 操作并不简单地在原文件的末尾对应的 offset 处开始写入数据，而是通过选择一个 offset。最后该被选择的 offset 会返回给 Client，代表此次 record 的起始数据偏移量。由于 GFS 对于 Record Append 采用的是 at least once 的消息通信模型，虽然确保此次写操作成功，但是可能造成重复写数据。</li>
</ul>
<p>在一系列成功的修改操作以后，被修改的文件区域的状态是 defined 并包含最后一次修改的写内容。GFS 通过以下两种方式实现这一目标：</p>
<ul>
<li>在所有写操作相关的 replicas 上以同一顺序给 chunk 进行修改；</li>
<li>使用 chunk version numbers 去检测 replicas 上的数据是否已经 stale，这种情况可能是由于 chunkserver 出现暂时的 down；</li>
</ul>
<p>一旦一个 replicas 被判定为 stale，那么 GFS 就不会基于此 replicas 进行任何修改操作，客户机再向 Master 节点请求元数据时，也会自动滤除 stale replicas。并且 Master 通常会对 stale replicas 进行 garbage collected（垃圾回收）。</p>
<p>出现的相关问题：</p>
<ul>
<li>因为在客户机存在 cache 的缘故，客户机还是有机会从 stale replica 上读取文件数据。这个时间穿窗口取决于缓存的超时时间设置以及下一次打开同一文件的限制。但是就算使用 cache 读到了数据，也不会造成很大的问题。因为GFS 中的大多数文件都是 append-only，因此 stale replica 上的读仅仅是返回一个 premature end of chunk，也就是说没有包含最新的追加内容的 chunk，而不是被覆写了的数据（因为无法覆写）</li>
<li>Master 节点通过与所有 chunkserver 进行 regular handshake来检测出现故障的 chunkserver，通过 check summing 来检测数据是否损坏。一旦出现问题，GFS 会尽快地从有效的 replicas 上进行数据恢复。除非在 Master 节点检测到故障之前，3 块 replica 都出现故障才会导致不可逆的数据丢失。不过即使是这样，GFS 系统也不会不可用，而是会及实地给客户端回应数据出错的响应，而不是返回出错的数据。</li>
</ul>
<p>使用 GFS 的 client 可以使用一些简单的技术来达到 GFS 系统所支持的宽松一致性协议，比如：</p>
<ul>
<li>尽量选择 append 追加，而不是 overwrite 覆写，这是因为 GFS 仅仅保证 append 操作的一致性，但是覆写操作没有一致性保证；</li>
<li>写入数据时使用额外的校验信息，比如校验和（或者其他 hash 技术）；</li>
<li>可以加入额外的唯一标识符来去除因为 write at least once 而造成的重复数据（客户端在读取数据时如果发现唯一标识符已经读过了，那么就舍弃这部分数据）；</li>
</ul>
<h3 id="SYSTEM-INTERACTIONS"><a href="#SYSTEM-INTERACTIONS" class="headerlink" title="SYSTEM INTERACTIONS"></a>SYSTEM INTERACTIONS</h3><h4 id="Leases-and-Mutation-Order"><a href="#Leases-and-Mutation-Order" class="headerlink" title="Leases and Mutation Order"></a>Leases and Mutation Order</h4><p>修改操作包括追加和覆写，这两种写操作都会作用于所有的 replica，就是一个写指令对应于 3 个 chunk+replica 的 I/O 写。对于 chunk 的写操作涉及两种修改：</p>
<ul>
<li>在相关 chunkserver 上进行 I/O 写操作；</li>
<li>在 Master 节点上修改 metadata；</li>
</ul>
<p><img src="/2021/12/15/google-file-system-md/gfs-write-control.jpg"></p>
<ul>
<li>Step1：client 向 Master 查询哪一个 Chunk Server 持有要进行写操作的 Chunk 的 Lease；</li>
<li>Step2：Master 回应 primary 的标识符（包括地址）以及其他 replicas 的地址。client 接收后将此回应进行缓存，其会在 primary 不可达或者其不再持有 lease 时再次向 Master 查询；</li>
<li>Step3：client 向所有的 replicas 都推送数据，注意此时client 可以依靠任意顺序进行推送数据，并没有要求此时必须先给 primary 推送数据。所有的 chunkserver(replicas) 都会将 client 推送来的数据存放在内置的 LRU buffer cacahe，缓存中的数据直到被使用或者超时才会被释放。</li>
<li>Step4：只要 replicas 回复已经接收到了所有数据，那么 Client 就会发送一个 write 指令给 primary 节点，primary 节点为多个写操作指定执行的顺序（写操作可能来自于多个 Client），然后将此顺序应用于其本地 I/O 写操作。</li>
<li>Step5：primary 节点将写操作请求转发给其他两个 replica，它们都将按照 primary 的顺序执行本地的 I/O 写操作；</li>
<li>Step6：secondaries 返回写成功给 primary 节点；</li>
<li>Step7：Primary 响应 client，并返回该过程中发生的错误。这里的错误不仅是 Primary 的写操作错误，还包括其他两个 replica 的写操作错误。如果 primary 自身发生错误，其就不会向其他两个 replica 节点进行转发。另一方面，如果 Client 收到写失败响应，那么其会重新进行写操作尝试，即重新开始 3-7 步。</li>
</ul>
<h4 id="Data-Flow"><a href="#Data-Flow" class="headerlink" title="Data Flow"></a>Data Flow</h4><p>关于数据流，GFS 的目标是将数据流和控制流进行解耦，解耦的方式就是以线型的方式进行传输数据。具体来说就是 Client 负责给 primary 传输写入的数据，而 primary 负责给下一个 replica 传输数据，而下一个 replica 又负责给下下一个 replica 传输数据。这种方式能充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，并最小化通过所有数据的延迟。</p>
<p>具体来说，每一个节点都会将数据转发给最近且还没有收到数据的节点，节点通过 IP 地址来估算两个节点之间的链路距离。</p>
<h4 id="Atomic-Record-Appends"><a href="#Atomic-Record-Appends" class="headerlink" title="Atomic Record Appends"></a>Atomic Record Appends</h4><p>record append 是指向已经存储的文件的末尾追加数据，因此客户端并不需要像读操作那样提供一个数据范围，因为 record append 操作总是在文件末尾追加数据，这个地址偏移量应当交给 chunksever 来确定。</p>
<p>Record append 操作还涉及 primary 的选择步骤：</p>
<ul>
<li>Master 节点在接受到修改请求时，会找此 file 文件最后一个 chunk 的 up-to-date 版本（最新版本），最新版本号应当等于 Master 节点的版本号；</li>
<li>Master 在选择好 primary 节点后递增当前 chunk 的 chunk version，并通过 Master 的持久化机制持久化；</li>
<li>Primary 与其他 chunkserver，发送修改此 chunk 的版本号的通知，而节点接收到通知后会修改版本号，然后持久化；</li>
<li>Primary 然后开始选择 file 最后一个文件的 chunk 的末尾 offset 开始写入数据，写入后将此消息转发给其他 chunkserver，它们也对相同的 chunk 在 offset 处写入数据</li>
</ul>
<p><strong>如果向 file 追加的数据超过了 chunk 剩余容量怎么办？</strong></p>
<ul>
<li>首先，这是一个经常发生的问题，因为 record append 操作实际上能一次添加的数据大小是被限制的，大小为 chunksize（64 MB）的 1/4，因此在大多数常见下，向 chunk append 数据并不会超出 64 MB 大小的限制；</li>
<li>其次，如果真的发生了这个问题，那么 Primary 节点还是会向该 chunk append 数据，直到达到 64MB 大小上限，然后通知其他两个 replicas 执行相同的操作。最后响应客户端，告知客户端创建新 chunk 再继续填充</li>
</ul>
<p><strong>At least once 机制引发的问题</strong></p>
<p>这里我们发现 primary 与 secondary_1 的 chunk 的数据是一致的，但是都出现了额外的一次 B 数据块的写入。secondary_2 的数据块和前面两个都不同，主要问题是由于在第一次写入 B secondary_2 因为网络问题没能写入数据块 B。</p>
<ul>
<li>三个 chunk 中的任何一个上的 record append 若执行失败，primary 会负责告知client 数据块写入错误。</li>
<li>但是 Primary 在告知 client 之前已经成功地按照顺序正确写入了；</li>
<li>客户端可以选择再次发起数据块 B 的重写请求，但是这个操作并不是及时的，因为 A、B、C 已经写入。因此最后我们能看到，数据块 B 实际上处于整个 chunk 的文件末尾；</li>
</ul>
<p><img src="/2021/12/15/google-file-system-md/gfs-append-problem.jpg"></p>
<p>最后 3 个 chunk 的 chunk version 实际上是一样的，因此 Master 以及 chunkserver 并不能辨别出哪一个 chunk 存在数据问题。如果在如上操作后客户端发起此 chunk 的请求，那么最终取决于 client 在哪一个 chunkserver 读取 chunk 数据 。</p>
<p>因此客户端在读取 chunk 数据时就应当考虑到存在这个问题，主动通过其他机制来避免（比如为每一次的 append 操作的数据也使用上 metadata，比如字长信息的描述）。客户端在读取此 chunk 时，可以通过校验和避免得到失败写入的数据，通过唯一标识符避免读到重复数据</p>
<h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><ul>
<li>Client —&gt; Master 请求：file name + range(或者说是 offset，总之是客户端打算读取的字节范围)；</li>
<li>Master —-&gt; Client 响应：chunk handle + a list of server</li>
<li>client 在 a list of server 中跳出一个 chunkserver，GFS 论文指出，通过 chunkserver 的 IP 地址，能够 guess 出距离当前 client 最近的 chunksver，然后 Client 会优先向最近的 chunkserver 请求数据读取；</li>
<li>chunkserver 收到数据读取请求后，根据 clinet 发来的 chunk hanle 进行磁盘 I/O 最终返回数据给 client；</li>
</ul>
<p>但是存在更复杂的情况，比如：</p>
<ul>
<li>客户端索要的数据大于固定大小 chunk 的最大数据量：64 MB；</li>
<li>客户端索要的数据虽然在 64 MB 以内，但是其字节范围恰好跨越着两个 chunk 之间的边界；</li>
</ul>
<p>事实上，客户端依赖于 GFS lib 进行数据读取，因此对客户端而言，其甚至不知道 File 底层被 chunk 存储。对于第一种情况，在逻辑上可以认为 GFS lib 底层会向 Master 节点索要两次 chunk 的 metadata 数据（实际上仅仅需要查询一次），然后 GFS lib 底层又会分别与两个 chunkserver 进行数据传输。对于第二种情况，GFS lib 在发送请求时并不会知道此次读取涉及两个 chunk 的数据读取，但是其会接收到来自 Master 两个 chunk 的 metadata，接着也会分别和与两个 chunkserver 进行数据传输。</p>
<h4 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h4><p>Snapshot(快照)操作的含义就是为文件创建一个副本或者直接为一个目录树创建副本（有多个文件），在 GFS 中以非常快速的方式进行，并且尽量会减少对于正在进行的写操作的影响。</p>
<p><strong>GFS 使用 standard copy-on-write 技术来实现快照</strong>，其实现方式是：</p>
<ul>
<li>当一个 Master 接收到一个 snapshot 请求，它首先会 revoke 对拷贝涉及的 chunk 对应的 lease，这用于确保后续写操作必须经过 Master 的重新进行交互，以便于 Master 有机会先创建 chunk 的副本。</li>
<li>当 lease 撤回或者过期后，Master 首先会将操作日志记录到磁盘，然后通过复制源文件以及目录树的 metadata 来将日志记录应用到内存中的状态。</li>
<li>当有关 client 请求 Master 对这些 chunk 进行写操作时，Master 通过这些 chunk 上的引用计数大于 1，于是 Master 就会为这些 chunk 创建相关的 handler，然后通知拥有这些 chunk 的 chunkserver 创建数据相同的 chunk（这种方式不再 Master 上进行复制，目的是节约 Master 带宽与内存）。</li>
<li>最后客户端新的写请求将直接作用于这些新创建的 chunk 上，同时也会被颁发新的 lease；</li>
</ul>
<h3 id="Master-operation"><a href="#Master-operation" class="headerlink" title="Master operation"></a>Master operation</h3><p>Master 节点负责的工作有：</p>
<ul>
<li>namespace manager；</li>
<li>管理整个系统中的所有 chunk replicas：<ul>
<li>chunk 实际存储在哪里；</li>
<li>创建新的 chunk 和 replica；</li>
<li>协调系统的各种操作（比如读、写、快照等），保证 chunk 正确且有效地进行备份；</li>
<li>管理 chunkserver 之间的负载均衡；</li>
<li>回收没有被使用的存储空间</li>
</ul>
</li>
</ul>
<h4 id="Namespcace-management-and-locking"><a href="#Namespcace-management-and-locking" class="headerlink" title="Namespcace management and locking"></a>Namespcace management and locking</h4><p>Master 节点有很多操作都需要执行很长时间，比如：snapshot 操作必须向 chunkserver 撤回 snapshot 涉及的所有 chunk 的 lease。我们并不希望这些耗时的操作会影响 Master 节点的其他操作。出于这个目的，我们给 namespace 上锁来实现可以同时进行多个操作以及确保操作的正确串行执行顺序。</p>
<p>不同于其他传统的文件系统，GFS 并没有为每一个目录创建一个用于记录当前目录拥有哪些文件的数据结构，也不支持文件和目录的别名。GFS 逻辑上将其 namesapace 当做一个查询表，用于将 full pathname（要么是一个 absolute file name，要么是一个 absolute directory name） 映射为 metadata。如果使用 prefix compression（前缀压缩），那么这个表可以在内存中被高效地表示。在 namespace 树中的每一个节点对应一个 full pathname，都拥有一个与之相关联的 read-write lock(读写锁)。</p>
<p><strong>写操作涉及的文件/目录的锁获取结构有如下的规律</strong>：</p>
<ul>
<li><strong>最底层的文件/目录一定是获得写锁</strong>，因此 Master 其他操作不能同时修改同一的底层文件/目录，因为<strong>写锁是排它锁</strong>；</li>
<li><strong>除了最底层的文件/目录，其他所有父目录、祖父目录仅仅需要获得读锁，读锁是一种共享锁</strong>。因此 Master 其他操作并不能修改父目录，比如将 <code>/home/user</code> 改为 <code>/home/usr</code> 是会被阻塞的，</li>
</ul>
<h4 id="Replica-Placement"><a href="#Replica-Placement" class="headerlink" title="Replica Placement"></a>Replica Placement</h4><p>GFS 系统是一个分布式系统，一个 GFS 集群通常会有成百上千个 chunkserver，它们分布在很多 machine rack。每一个 chunkserver 可以被成百上千个其他位于同一或不同的机架上的 chunkserver 访问。在两个不同机架上进行通信的 chunkserver 需要通过交换机。此外，机架的输入输出带宽可能小于机架内所有 chunkserver 的总带宽。这些多级分布对数据的可伸缩性、可靠性和可用性提出了挑战。</p>
<p>chunk replica placement policy 有两个目的：</p>
<ul>
<li>最大化数据 reliability（可靠性）和 availability（可用性）；</li>
<li>最大化 network bandwidth utilization（网络带宽利用率）；</li>
</ul>
<p>为了达到上述目的，在 chunkserver 和 chunkserver 之间传播 replicas 是不够的，这仅仅能够在机器或磁盘故障时保障可靠性和可用性，且会给单个 rack 带来带宽使用上的压力，因为读取数据时，无论客户端最终选择哪一个 replicas，最终都是消耗着同一个 replicas 的带宽。我们还需要在不同的 racks 上传输 chunk replicas，这能够在一整个机架故障时都能够确保可靠性和可用性（简单来说就是跨 rack 存储 replicas）。如果 chunk replicas 分布存储在不同机架上的 chunkserver 上，可以降低每一个 rack 提供 replicas 读写时的带宽压力，相同于多个机架平均了带宽压力。</p>
<h4 id="Creation-Re-replication-Rebalancing"><a href="#Creation-Re-replication-Rebalancing" class="headerlink" title="Creation, Re-replication, Rebalancing"></a>Creation, Re-replication, Rebalancing</h4><p>chunk replicas 出于三个原因被创建：</p>
<ul>
<li>chunk reation；</li>
<li>Re-replication；</li>
<li>reblancing；</li>
</ul>
<p>当 Master 节点创建了一个 chunk，它负责确定将这个 initially empty replicas 放置到哪里，它鉴于以下几个因素进行判断：</p>
<ul>
<li>将 replica 放置于磁盘空间利用率低于平均水平的 chunkserver，这样能够使所有 chunkserver 在磁盘利用率上保持一致性；</li>
<li>限制 chunkserver 上创建的 chunk 的频率，虽然仅仅创建一个 chunk 代价不高，但是它通常是即将出现大量写操作的前兆</li>
<li>正如 4.2 节所谈到的，我们期望将 replicas of chunk 分散放置在不同的 rack 上。</li>
</ul>
<p>一旦可用的 replicas 数量下降到用户预设值（默认为 3），那么 Master 就会开始 re-replicate chunk 操作。这可能由于如下的原因造成：</p>
<ul>
<li>chunkserver unavailable（不可用），比如它给 Master 发送如下状态信息：它的 replica 崩溃了、某一个磁盘不可读。</li>
<li>程序员修改了配置，动态增加了 replication 的个数的要求；</li>
</ul>
<p>当 chunk 需要被 re-replicated 时，Master 通过以下因素来确定执行优先级：</p>
<ol>
<li>根据距离 replication goal 的配置的距离来确定优先级。比如默认三个 replicas，有一组 replicas 中有两个出现了不可用，而另一组仅仅只有一个出现了不可用，因此前者比后有优先级高；</li>
<li>最近活动的文件（被读、被写）比最近删除的文件的 chunk 有更高的优先级；</li>
<li>如果 chunk 的读写可能阻塞客户端，那么该 chunk 将有较高的优先级，这能够减少 chunk 故障时对使用 GFS 的应用程序的影响；</li>
</ol>
<p>当 chunk 需要被 re-replicated 时，Master 通过以下因素来确定执行优先级：</p>
<ol>
<li>根据 replication goal 来确定优先级（和预期相差越多优先级越高）。比如默认三个 replicas，有一组 replicas 中有两个出现了不可用，而另一组仅仅只有一个出现了不可用，因此前者比后有优先级高；</li>
<li>最近活动的文件（被读、被写）比最近删除的文件的 chunk 有更高的优先级（涉及垃圾回收）；</li>
<li>如果 chunk 的读写可能阻塞客户端，那么该 chunk 将有较高的优先级，这能够减少 chunk 故障时对应用程序的影响；</li>
</ol>
<p>Master 节点选择优先级最高的 chunk 先进行 clone（克隆），具体做法是通知相关 chunkserver 直接从存在可用 replica 的 chunkserver 上进行 chunk 数据的拷贝。</p>
<p><strong>注意</strong>：为了防止因为过多的 clone 操作占用过多的系统带宽，Master 节点既会限制chunkserver 进行的 clone 数量，又会限制整个 GFS 集群同时进行的 clone 操作数量。而且 chunkserver 自身也会对消耗在 clone 操作上的带宽占比，其方式是限制对复制源的请求数量。</p>
<p>Master 的 rebalancing 机制是指：Master 节点会负责检查当前 replica 的分布，然后会将相关 replicas 移动到更好的磁盘位置。通过这个机制 Master 节点能将一个新加入的 chunkserver <strong>逐渐</strong>填充，而不是在其刚加入时用大量的写操作来填充它。新的 replicas 的放置原理和上面两种类似。当然 Master 还要选择从哪一个 chunkserver 上删除相关 replicas，以便将其移动到磁盘空闲的 chunksever 上，选择依据是挑选磁盘空闲水平低于平均值的 chunkserver。</p>
<h4 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h4><p>当 client 向 Master 节点发出一个删除文件请求后，GFS 并不会立即回收文件的物理磁盘存储空间。GFS 提供了一个 regular(定期的)的垃圾收集机制，用于回收 file 和 chunk 级别的物理磁盘空间，可以见得这种垃圾回收机制属于 lazily(懒) 回收机制。</p>
<p>当一个文件被应用删除时，Master 节点会将此删除操作马上写入日志系统（Master 的其他写操作也都是如此）。但是 Master 并不会马上向相关 chunkserver 发出删除文件的请求，而是将文件重命名为 hadden name（隐藏名），文件的重命名工作仅仅在 Master 的 namespace 中进行，此名字包含接收到删除指令的时间戳。</p>
<p>在 Master 定期对 namespcae 的扫描过程中，其会移除所有距删除时间戳 3 天以上的 hidden files，3 天超时时间可以通过配置修改。在 3 天之内，hidden files 还是可以通过 hidden name 进行读取，不过原名是不行了。并且可以将文件恢复为正常名而撤销删除操作。</p>
<p>当 hidden file 从 namespace 中移除后，该文件在 Master 节点中的所有 metadata 都被移除了，然后所有的 chunkserver 会删除在磁盘上的相关文件。</p>
<p>Master 在对 namespace 的定时扫描中，如果发现那些 orphaned chunks（指那些不能从任何 file 上进行读的 chunk，类似于 Java 中没有被任何引用的实例），然后会从存储中删除这些 chunk 的 metadata 数据。ChunkServer 在和 Master 定时的心跳消息中，ChunkServer 会汇报其拥有的 chunks 的信息，然后 Master 节点的响应中会有关于哪些 chunk 没有在 Master 内存中存储 metadata 数据，ChunkServer 受到此消息后会检查哪些 chunk 实际上是不被 Master 节点引用的，然后会负责删除磁盘上没被引用的 chunk。</p>
<p><strong>Discussion 讨论</strong></p>
<p>与及时删除相比，这种定时且懒惰的垃圾收集方法有如下的优点：</p>
<ul>
<li><p>首先，这种方式简单可靠。</p>
<ul>
<li>chunk 的创建可能会在一些节点上成功，但是在另外的部分节点上失败，因此会留下一些 Master 节点没有记录的 chunk 数据。不过没有关系，因为 chunkserver 会主动告知其有哪些 chunk，Master 在发现其没有记录此 chunk 信息后，会告知 chunkserver，chunkserver 接下来就会删除这些重复创建的 chunk 数据；</li>
<li>chunk 的删除指令并不一定能够送达到 chunkserver，在 GFS 提供的方案中，因为会有定时的 heartbeat 机制，因此 Master 节点最终一定能告知 chunkserver 删除相关 chunk。但是如果不使用这套方案，那么 Master 必须依赖于额外的通信来确保 chunkserver 的确删除了相关 chunk；</li>
</ul>
</li>
<li><p>其次，这种垃圾回收机制能够平均化垃圾回收成本。</p>
<p>其定时回收机制依赖于定时的 namespace 扫描以及定时的 heartbeat 通信，因此垃圾回收最终是会分批进行的，而不会集中进行。而且只有在 Master 空闲的时候才会扫描 namespace，因此 Master 并不会因为垃圾回收而有过多的负担。</p>
</li>
<li><p>最后，这种 lazily 的垃圾回收机制可以防止意外删除，默认三天的超时删除机制为不可逆删除提供了额外的安全保障。</p>
</li>
</ul>
<p>延迟回收会阻碍用户调优存储空间的使用，特别是当存储空间比较紧缺的时候。当应用程序重复创建和删除临时文件时，释放的存储空间不能马上重用。我们通过显式的再次删除一个已经被删除的文件的方式加速空间回收的速度。我们允许用户为命名空间的不同部分设定不同的复制和回收策略。例如，用户可以指定某些目录树下面的文件不做复制，删除的文件被即时的、不可恢复的从文件系统移除。 </p>
<h4 id="Stale-Replica-Detection"><a href="#Stale-Replica-Detection" class="headerlink" title="Stale Replica Detection"></a>Stale Replica Detection</h4><p>当 chunkserver 故障了或者因为宕机没能够正确地实施写操作，那么 Chunk replicas 的状态就变为 stale。Master 节点为每一个 chunk 维护一个 chunk verison nember（chunk 版本号）来辨别哪些 chunk 是 up-to-date，哪些 chunk 是 stale。</p>
<p>当 Master 赋予一个 chunk 新的租赁时，其就会使 chunk version 自增，并将此版本号告知其他 replicas。</p>
<p>也就是说 Master 节点、primary 节点的 chunk version 和其他 chunkserver 节点的 chunk 的 chunk version 会保持一致。</p>
<p>版本号的通知优先于写操作执行前。如果其他 replica 此时不可用，那么这个 chunk version 的通知就无法到，因此其 chunk version 就不会增加。那么当此 Chunkserver 重启后的心跳消息中就会包含此 chunk version 信息，Master 就能借此发现 ChunkServer 拥有 stale 的 replica。如果 Master 发现高于自己记录的 chunk version number，那么 Master 会认为自己在授予 lease 时发生了错误，然后将高版本的 chunk version number 视为最新版本。</p>
<p>Master 节点在定时的垃圾收集任务中删除 stale replicas，当客户端对 stale chunk 请求时，Master 节点会发出此 chunk 不存在的响应。另外一重保障措施是，Master节点在通 知客户机哪个Chunk服务器持有租约、或者指示Chunk服务器从哪个Chunk服务器进行克隆时，消息中 都附带了Chunk的版本号。客户机或者Chunk服务器在执行操作时都会验证版本号以确保总是访问当前版本的数据。</p>
<h3 id="FAULT-TOLERANCE-AND-DIAGNOSIS"><a href="#FAULT-TOLERANCE-AND-DIAGNOSIS" class="headerlink" title="FAULT TOLERANCE AND DIAGNOSIS"></a>FAULT TOLERANCE AND DIAGNOSIS</h3><p>对于 GFS 系统设计的最大挑战是需要处理频繁的组件故障，组件的质量一般以及数量众多一起提高了挑战难度。我们总是不能完全信任主机与硬盘，组件的故障可以引发很严重的问题，比如 GFS 系统不可用，甚至是错误的数据。下面，我们将讨论我们如何来解决这些挑战，如何在错误不可避免地发生时进行问题诊断。</p>
<h4 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h4><p>在拥有着成千上百台主机的 GFS 集群中，在任何时刻都可能存在部分主机不可用。我们使用一些简单但是有效的策略就能保持整个系统的总体的高可用性，这两个策略是：fast recovery 与 replication。</p>
<p><strong>Fast Recovery</strong></p>
<p>Master 以及 chunkserver 节点不管出于何种原因故障，都能在几秒内恢复故障前的状态。GFS 并不对主机的正常关闭和异常关闭进行区别看待，事实上本身服务器本身就会日常地通过 kill 进行来进行关闭。客户端和其他服务器会因为当前 server 发生故障而发生请求未处理的超时情况，不过因为当前 Server 具备快速重启的能力，因此它们通常的做法就是重发请求。</p>
<p><strong>Chunk Replication</strong></p>
<p>就如之前提到的，每一个 chunk 会被复制到多个分布于不同机架上的 chunkserver 上。我们可以为 namespace 不同的区域指定不同的 replication 级别，默认的级别就是 3，即一共三份，2 份为额外的 replica。</p>
<p><strong>Master Replication</strong></p>
<p>Master 使用日志系统以及 checkpoint 来确保 Master 状态信息的可靠性，这一点和 MySQL 的日志系统是类似的。只有在内存快照被刷新到本地磁盘以及其他主机上的磁盘上时，才会认为修改状态提交了（类似于 MySQL 事务系统）。</p>
<p>另外 Master 还提供你 shadow Master 节点，这些节点在 Master 宕机时还能提供对文件系统的<strong>只读</strong>访问，但是注意，这里用词为 “shadow” 而不是 “mirror”，前者允许 shadow Master 的状态略微滞后于 Master 节点（通常延迟限制于几分之一秒内），后者要求必须保持一致。GFS 使用 shadow Master 节点在客户端不在意读到陈旧内容时能够很好的服务。不过大多数修改都是 append，因此客户端至多读不到新 append 的数据，而大概率不会读到错误的旧数据。</p>
<p>shadow Master 节点通过读取 Master 日志系统的 replicas 来进行状态的更新，然后根据日志的先后顺序执行操作，但是注意 shadow Master 节点并不参与和其他 chunkserver 进行通信，比如并不存在心跳机制</p>
<h4 id="Data-Integrity"><a href="#Data-Integrity" class="headerlink" title="Data Integrity"></a>Data Integrity</h4><p>chunkserver 使用 checksum 来检测存储的数据是否损坏</p>
<p>我们可以通过从其他 replicas 中读取数据来使损坏的数据恢复，但是通过比较 chunkserver 之间数据差别进行恢复是不切实际的。此即使发现了数据的不同，可能也不是因此磁盘损坏而导致的错误，而可能是因为 at least once 的机制导致了 append 数据多次。因此每一个 chunkserver 都必须维护校验和，并能够进行独立的副本完整性验证。</p>
<p>Chunkserver 中的每一个 chunk（大块） 被分为 64KB 大小的 block（小块），这意味着默认情况下一个 chunk 对应 1k 个 block。每一个小块都有对应的 32 位校验和。注意，校验和数据和用户数据分开存储，和其他元数据一样，一起保存在内存中，并最终通过日志系统持久化。</p>
<p>客户端向 chunkserver 发送读取数据的请求时，chunkserver 首先会对读操作涉及的所有 block 块进行校验。如果 chunkserver 发现校验和和数据不匹配，那么就会向请求者返回一个错误，同时还向 Master 节点报告错误。请求者接收到此响应后，会从其他 replica 上读取数据，Master 接收到此响应后，将从另一个 replica 上克隆数据块，并指示 chunkserver 删除它的 replica。</p>
<p>校验和机制对读性能的影响并不大，原因在于：</p>
<ul>
<li>客户端的大多数读操作至少涉及几个 block，我们只需要读取和校验相对少量的额外数据进行验证；</li>
<li>GFS 客户代码通过对齐读 checksum block boundaries，减少了开销；</li>
<li>chunkserver 上的 checksum 查找以及比较不需要任何 I/O 参与，校验和计算通常包含于 I/O 数据的读取；</li>
</ul>
<p>Checksum 机制对于 Record Append 操作进行了优化，我们只需要增加最后部分 block 的 checksum 即可。即使最后一个部分校验和块已经损坏，而我们现在没有检测到它，新的校验和值也不会与存储的数据匹配，当下一次读取块时，还是会被检测到损坏。但是覆写操作的校验和可能会失效，因为 GFS 并没有进行特别的优化。</p>
<p>另外，chunkservers 在空闲期间可以扫描和验证非活动 chunk 的内容。通过这种机制能够对很少被读取的 chunk 也进行数据是否损坏的检测，一旦检测到损坏，主服务器可以创建一个新的未损坏的副本并删除损坏的副本。这样可以防止 Master 错误认为对于一个不被经常读取的 chunk 有着符合配置数量要求的 replicas。</p>
<h4 id="Diagnostic-Tools"><a href="#Diagnostic-Tools" class="headerlink" title="Diagnostic Tools"></a>Diagnostic Tools</h4><p>如果日志系统是 extensive 和 detailed 的，那么就能够帮助程序员进行问题隔离、调试、性能分析。日志的成本因为其顺序写的特性通常并不高，但是有极大的好处。</p>
<p>GFS 服务器会生成 diagnostic(诊断)日志（这里的日志系统是 chunkserver 的日志系统），记录一些比较重要的事件，比如：</p>
<ul>
<li>chunksever 上下的移动；</li>
<li>RPC 请求和响应；</li>
</ul>
<p>除了读取或写入的文件数据外，RPC 日志包括通过网络发送的确切请求和响应。通过将请求与响应进行匹配，并比较不同机器上的 RPC 记录，我们可以重构整个交互历史以诊断问题。这些日志还可以作为负载测试和性能分析的跟踪。</p>
<p>日志对性能的影响很小(远小于它带来的好处)，因为这些日志的写入方式是顺序的、异步的。最近发生 的事件日志保存在内存中，可用于持续不断的在线监控。 </p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/12/07/CMakeTutorial/" rel="prev" title="CMakeTutorial">
      <i class="fa fa-chevron-left"></i> CMakeTutorial
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Google-File-System-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB"><span class="nav-number">1.</span> <span class="nav-text">Google File System 论文阅读</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Background"><span class="nav-number">1.1.1.</span> <span class="nav-text">Background</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Design-Overview"><span class="nav-number">1.2.</span> <span class="nav-text">Design Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Assumptions"><span class="nav-number">1.2.1.</span> <span class="nav-text">Assumptions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interface"><span class="nav-number">1.2.2.</span> <span class="nav-text">Interface</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Architecture"><span class="nav-number">1.2.3.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Chunk-size"><span class="nav-number">1.2.4.</span> <span class="nav-text">Chunk size</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Single-Master"><span class="nav-number">1.2.5.</span> <span class="nav-text">Single Master</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Metadata"><span class="nav-number">1.2.6.</span> <span class="nav-text">Metadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Consistency-Model"><span class="nav-number">1.2.7.</span> <span class="nav-text">Consistency Model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SYSTEM-INTERACTIONS"><span class="nav-number">1.3.</span> <span class="nav-text">SYSTEM INTERACTIONS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Leases-and-Mutation-Order"><span class="nav-number">1.3.1.</span> <span class="nav-text">Leases and Mutation Order</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Flow"><span class="nav-number">1.3.2.</span> <span class="nav-text">Data Flow</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Atomic-Record-Appends"><span class="nav-number">1.3.3.</span> <span class="nav-text">Atomic Record Appends</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Read"><span class="nav-number">1.3.4.</span> <span class="nav-text">Read</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Snapshot"><span class="nav-number">1.3.5.</span> <span class="nav-text">Snapshot</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Master-operation"><span class="nav-number">1.4.</span> <span class="nav-text">Master operation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Namespcace-management-and-locking"><span class="nav-number">1.4.1.</span> <span class="nav-text">Namespcace management and locking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Replica-Placement"><span class="nav-number">1.4.2.</span> <span class="nav-text">Replica Placement</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Creation-Re-replication-Rebalancing"><span class="nav-number">1.4.3.</span> <span class="nav-text">Creation, Re-replication, Rebalancing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Garbage-Collection"><span class="nav-number">1.4.4.</span> <span class="nav-text">Garbage Collection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stale-Replica-Detection"><span class="nav-number">1.4.5.</span> <span class="nav-text">Stale Replica Detection</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FAULT-TOLERANCE-AND-DIAGNOSIS"><span class="nav-number">1.5.</span> <span class="nav-text">FAULT TOLERANCE AND DIAGNOSIS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#High-Availability"><span class="nav-number">1.5.1.</span> <span class="nav-text">High Availability</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Integrity"><span class="nav-number">1.5.2.</span> <span class="nav-text">Data Integrity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Diagnostic-Tools"><span class="nav-number">1.5.3.</span> <span class="nav-text">Diagnostic Tools</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="l1nkr"
      src="/images/ye.jpg">
  <p class="site-author-name" itemprop="name">l1nkr</p>
  <div class="site-description" itemprop="description">Work it harder. Make it better</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/l1nkr" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;l1nkr" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lingfeng9911@gmail.com" title="E-Mail → mailto:lingfeng9911@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">l1nkr</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
